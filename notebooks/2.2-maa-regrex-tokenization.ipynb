{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\coding\\text-proc-ml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\coding\\text-proc-ml\\.venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change.\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded.\n",
    "%autoreload 2\n",
    "\n",
    "# Смена рабочей папки.\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZE_RE = re.compile(r\"[а-яё]+|-?\\d*[.,]?\\d+|\\S\", re.I)\n",
    "# -?\\d*[.,]?\\d+\n",
    "\n",
    "\n",
    "def tokenize(txt):\n",
    "    return TOKENIZE_RE.findall(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тестовое сообщение\n",
      "мама мыла раму .\n",
      "контактный телефон : 123123 .\n",
      "что - нибудь надо придумать .\n",
      "значение числа е = 2.7182 .\n",
      "демон 123 , как тебя зовут в реале ?\n",
      "-1 -.15 = -1.15\n",
      "- 1 - .15 = -1.15\n",
      "какого ; % : ? * тут происходит ?\n"
     ]
    }
   ],
   "source": [
    "text_corpus = [\n",
    "    \"Тестовое сообщение\",\n",
    "    \"Мама мыла раму.\",\n",
    "    \"Контактный телефон: 123123.\",\n",
    "    \"Что-нибудь надо придумать.\",\n",
    "    \"Значение числа Е=2.7182.\",\n",
    "    \"Демон123, как тебя зовут в реале?\",\n",
    "    \"-1-.15=-1.15\",\n",
    "    \"- 1 - .15 = -1.15\",\n",
    "    \"Какого ;%:?* тут происходит?\",\n",
    "]\n",
    "for text_value in text_corpus:\n",
    "    print(\" \".join(tokenize(text_value.strip().lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.5 Вектор весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dlnlputils.data import (\n",
    "    tokenize_text_simple_regex,\n",
    "    tokenize_corpus,\n",
    "    build_vocabulary,\n",
    "    vectorize_texts,\n",
    "    SparseFeaturesDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus_for_w = [\n",
    "    \"Казнить нельзя, помиловать. Нельзя наказывать.\",\n",
    "    \"Казнить, нельзя помиловать. Нельзя освободить.\",\n",
    "    \"Нельзя не помиловать.\",\n",
    "    \"Обязательно освободить.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = tokenize_corpus(text_corpus_for_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['казнить', 'нельзя', 'помиловать', 'нельзя', 'наказывать'],\n",
       " ['казнить', 'нельзя', 'помиловать', 'нельзя', 'освободить'],\n",
       " ['нельзя', 'не', 'помиловать'],\n",
       " ['обязательно', 'освободить']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных токенов 7\n",
      "[('нельзя', 0), ('помиловать', 1), ('казнить', 2), ('освободить', 3), ('наказывать', 4), ('не', 5), ('обязательно', 6)]\n"
     ]
    }
   ],
   "source": [
    "MAX_DF = 1\n",
    "MIN_COUNT = 0\n",
    "vocabulary, word_doc_freq = build_vocabulary(\n",
    "    corpus_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT\n",
    ")\n",
    "UNIQUE_WORDS_N = len(vocabulary)\n",
    "print(\"Количество уникальных токенов\", UNIQUE_WORDS_N)\n",
    "print(list(vocabulary.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'нельзя': 0,\n",
       " 'помиловать': 1,\n",
       " 'казнить': 2,\n",
       " 'освободить': 3,\n",
       " 'наказывать': 4,\n",
       " 'не': 5,\n",
       " 'обязательно': 6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = [(word, word_doc_freq[i]) for i, (word, _) in enumerate(vocabulary.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75, 0.75, 0.5 , 0.5 , 0.25, 0.25, 0.25], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "наказывать не обязательно казнить освободить нельзя помиловать\n",
      "0.25 0.25 0.25 0.5 0.5 0.75 0.75\n"
     ]
    }
   ],
   "source": [
    "answer = sorted(word_df, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "\n",
    "answer_1 = []\n",
    "answer_2 = []\n",
    "\n",
    "for k, v in list(answer):\n",
    "    answer_1.append(k)\n",
    "    answer_2.append(str(v))\n",
    "\n",
    "print(\" \".join(answer_1))\n",
    "print(\" \".join(answer_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['казнить', 'нельзя', 'помиловать', 'нельзя', 'наказывать'],\n",
       " ['казнить', 'нельзя', 'помиловать', 'нельзя', 'освободить'],\n",
       " ['нельзя', 'не', 'помиловать'],\n",
       " ['обязательно', 'освободить']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZATION_MODE = \"tfidf\"\n",
    "vectors = vectorize_texts(\n",
    "    corpus_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 13 stored elements and shape (4, 7)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t0.26666656136512756\n",
      "  (0, 1)\t0.13333328068256378\n",
      "  (0, 2)\t0.19999991357326508\n",
      "  (0, 4)\t0.39999982714653015\n",
      "  (1, 0)\t0.26666656136512756\n",
      "  (1, 1)\t0.13333328068256378\n",
      "  (1, 2)\t0.19999991357326508\n",
      "  (1, 3)\t0.19999991357326508\n",
      "  (2, 0)\t0.22222213447093964\n",
      "  (2, 1)\t0.22222213447093964\n",
      "  (2, 5)\t0.6666663885116577\n",
      "  (3, 3)\t0.4999997615814209\n",
      "  (3, 6)\t0.9999995231628418\n"
     ]
    }
   ],
   "source": [
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.50 -0.50 -0.50 0.87 -0.76 0.60 0.16\n",
      "-0.50 -0.50 -0.50 0.87 0.18 0.60 0.16\n",
      "-0.50 1.50 -0.50 -0.87 -0.76 0.29 1.04\n",
      "-0.50 -0.50 1.50 -0.87 1.34 -1.48 -1.36\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    \"Казнить нельзя, помиловать. Нельзя наказывать.\",\n",
    "    \"Казнить, нельзя помиловать. Нельзя освободить.\",\n",
    "    \"Нельзя не помиловать.\",\n",
    "    \"Обязательно освободить.\",\n",
    "]\n",
    "\n",
    "# Получаем счетчики слов\n",
    "TF = CountVectorizer().fit_transform(corpus)\n",
    "\n",
    "# Строим IDF.\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False, use_idf=True)\n",
    "vectorizer.fit_transform(corpus)\n",
    "\n",
    "## из IDF  в DF\n",
    "word_doc_freq = 1 / np.exp(vectorizer.idf_ - 1)\n",
    "\n",
    "# TF нормируем и сглаживаем логарифмом\n",
    "TFIDF = np.log(TF.toarray() / TF.sum(axis=1) + 1) / word_doc_freq\n",
    "# Масштабируем признаки\n",
    "TFIDF = np.asarray(TFIDF)\n",
    "scaledTFIDF = StandardScaler().fit_transform(TFIDF)\n",
    "\n",
    "# Домножаем на np.sqrt((4-1)/4) для перевода из DDOF(0) в DDOF(1) для 4 текстов\n",
    "scaledTFIDF *= np.sqrt(3 / 4)\n",
    "\n",
    "# Вывод в порядке возрастания DF\n",
    "for l in scaledTFIDF[:, np.argsort(word_doc_freq)]:\n",
    "    print(\" \".join([\"%.2f\" % d for d in l]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
